{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda9d344-2afc-4b4e-980a-21467b1e990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Union\n",
    "import ollama\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16cd90bb-7e12-46a1-af9d-c122f6f9cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Union\n",
    "\n",
    "def extract_history_marker(text: str) -> Union[int, str, None]:\n",
    "    \"\"\"\n",
    "    Scans the entire `text` for:\n",
    "      1) Any occurrences of U+2002 (en-space) followed by a 4-digit year, returning the last one.\n",
    "      2) Otherwise, in the History section:\n",
    "         a) 'Acts YYYY'\n",
    "         b) 'Source: L. YYYY'\n",
    "      3) Otherwise '[Reserved.]' or '[Repealed.]'\n",
    "      4) Else None\n",
    "    \"\"\"\n",
    "    # 1) look for all ‚Äú\\u2002YYYY‚Äù matches and return the last, if any\n",
    "    u2002_years = re.findall(r'\\u2002(\\d{4})', text)\n",
    "    if u2002_years:\n",
    "        return int(u2002_years[-1])\n",
    "\n",
    "    # 2) isolate the History section (if present)\n",
    "    parts = re.split(r'History', text, maxsplit=1, flags=re.IGNORECASE)\n",
    "    history = parts[1] if len(parts) > 1 else text\n",
    "\n",
    "    # 3a) look for Acts YYYY\n",
    "    m = re.search(r'Acts\\s+(\\d{4})', history)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "\n",
    "    # 3b) look for Source: L. YYYY\n",
    "    m2 = re.search(r'Source:\\s*L\\.\\s*(\\d{4})', history)\n",
    "    if m2:\n",
    "        return int(m2.group(1))\n",
    "\n",
    "    u2002_years = re.findall(r'\\u2002(\\d{2})', text)\n",
    "    if u2002_years:\n",
    "        return int(u2002_years[-1])\n",
    "\n",
    "    # 2) isolate the History section (if present)\n",
    "    parts = re.split(r'History', text, maxsplit=1, flags=re.IGNORECASE)\n",
    "    history = parts[1] if len(parts) > 1 else text\n",
    "\n",
    "    # 3a) look for Acts YY\n",
    "    m = re.search(r'Acts\\s+(\\d{2})', history)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "\n",
    "    # 3b) look for Source: L. YY\n",
    "    m2 = re.search(r'Source:\\s*L\\.\\s*(\\d{2})', history)\n",
    "    if m2:\n",
    "        return int(m2.group(1))\n",
    "    \n",
    "\n",
    "    # 4) fall back to Reserved/Repealed markers\n",
    "    if re.search(r'\\[Reserved\\.\\]', history, re.IGNORECASE):\n",
    "        return \"Reserved\"\n",
    "    if re.search(r'\\[Repealed\\.\\]', history, re.IGNORECASE):\n",
    "        return \"Repealed\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def trim_to_body(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a statute text that has metadata up top, chop off everything\n",
    "    before the first occurrence of three consecutive newlines.\n",
    "\n",
    "    If no triple-newline is found, returns the original text.\n",
    "    \"\"\"\n",
    "    # Method 1: simple string find\n",
    "    marker = \"\\n\\n\\n\"\n",
    "    idx = text.find(marker)\n",
    "    if idx != -1:\n",
    "        return text[idx + len(marker):]\n",
    "\n",
    "    # Fallback: more flexible regex (handles CRLF or extra spaces)\n",
    "    parts = re.split(r'(?:\\r?\\n){3,}', text, maxsplit=1)\n",
    "    return parts[1] if len(parts) > 1 else text\n",
    "\n",
    "# def ollama_chat(text):\n",
    "#     response = ollama.chat(model='mistral', messages=[\n",
    "#       {\n",
    "#         'role': 'user',\n",
    "#         'content': \"You are a legislation analyst. \"\n",
    "#                 \"Does the following statute text contain useful information on renewable energy incentives? Please answer in one word yes or no:\"\n",
    "#                 f\"{text}\"\n",
    "#             ,\n",
    "#       },\n",
    "#     ])\n",
    "#     return response['message']['content']\n",
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# import ollama  # make sure your ollama python binding is imported\n",
    "\n",
    "def ollama_chat(text: str) -> str:\n",
    "    response = ollama.chat(model='mistral', messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': (\n",
    "                \"You are a legislation analyst. \"\n",
    "                \"Does the following statute text contain useful information on renewable energy incentives? \"\n",
    "                \"Please answer in one word yes or no:\\n\\n\"\n",
    "                f\"{text}\"\n",
    "            ),\n",
    "        },\n",
    "    ])\n",
    "    return response['message']['content']\n",
    "\n",
    "def batch_ollama_chat(texts: list[str], max_workers: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Run ollama_chat in parallel over a list of `texts`, using up to `max_workers` threads.\n",
    "    Returns a list of responses in the same order as `texts`.\n",
    "    \"\"\"\n",
    "    results: dict[int, str] = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # schedule all the calls, keep track of their index\n",
    "        future_to_index = {\n",
    "            executor.submit(ollama_chat, txt): idx\n",
    "            for idx, txt in enumerate(texts)\n",
    "        }\n",
    "        # as each completes, store result in the dict\n",
    "        for future in as_completed(future_to_index):\n",
    "            idx = future_to_index[future]\n",
    "            try:\n",
    "                results[idx] = future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"[!] Error in ollama_chat for item {idx}: {e}\")\n",
    "                results[idx] = \"\"  # fallback\n",
    "\n",
    "    # return answers in the original order\n",
    "    return [results[i] for i in range(len(texts))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0f1b8-b927-45fc-b063-da76fc1f129e",
   "metadata": {
    "collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_actual = []\n",
    "\n",
    "async def scrape_energy_all_pages() -> list[str]:\n",
    "    all_texts = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        # 1) Navigate & dismiss the popup\n",
    "        await page.goto(\n",
    "            \"https://advance.lexis.com/container?config=0345494EJAA5ZjE0MDIyYy1kNzZkLTRkNzktYTkxMS04YmJhNjBlNWUwYzYKAFBvZENhdGFsb2e4CaPI4cak6laXLCWyLBO9&crid=71f400f1-686d-4c50-8ecc-7711eca7c5a8\",\n",
    "            wait_until=\"networkidle\"\n",
    "        )\n",
    "        await page.click(\"input#btnagreeterms\", timeout=10000)\n",
    "\n",
    "        # 2) Search ‚Äúenergy‚Äù\n",
    "        await page.fill(\"textarea#searchTerms\", \"energy\")\n",
    "        await asyncio.gather(\n",
    "            page.press(\"textarea#searchTerms\", \"Enter\"),\n",
    "            page.wait_for_load_state(\"networkidle\")\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            # wait for this page‚Äôs hits to render\n",
    "            await page.wait_for_selector(\"li.usview\", timeout=20000)\n",
    "            rows = page.locator(\"li.usview\")\n",
    "            count = await rows.count()\n",
    "            print(f\"[+] Found {count} hits on this page\")\n",
    "\n",
    "            for i in range(count):\n",
    "                row = rows.nth(i)\n",
    "\n",
    "                # grab the teaser‚Äêpage link in <p class=\"min vis\">\n",
    "                link = row.locator(\"p.min.vis a\").first\n",
    "                await asyncio.gather(\n",
    "                    link.click(),\n",
    "                    page.wait_for_load_state(\"networkidle\")\n",
    "                )\n",
    "\n",
    "                # extract the full document text\n",
    "                await page.wait_for_selector(\"section#document\", timeout=10000)\n",
    "                full = await page.inner_text(\"section#document\")\n",
    "                all_texts.append(full)\n",
    "                docs_actual.append(full)\n",
    "\n",
    "                # go back to the hits list\n",
    "                await asyncio.gather(\n",
    "                    page.go_back(),\n",
    "                    page.wait_for_load_state(\"networkidle\")\n",
    "                )\n",
    "\n",
    "            # now try to click ‚ÄúNext‚Äù\n",
    "            await page.wait_for_timeout(1000)\n",
    "            page.wait_for_load_state(\"networkidle\", timeout=10000)\n",
    "            next_btn = page.locator(\"nav.pagination >> a:has-text('Next')\")\n",
    "            if await next_btn.count() and await next_btn.is_visible():\n",
    "                print(\"[+] Clicking Next ‚Üí\")\n",
    "                await asyncio.gather(\n",
    "                    next_btn.first.click(),\n",
    "                    page.wait_for_load_state(\"networkidle\")\n",
    "                )\n",
    "                print(\"[+] Now on the next page!\")\n",
    "                # pause for 1 second\n",
    "            await page.wait_for_timeout(1000)\n",
    "            page.wait_for_load_state(\"networkidle\", timeout=10000)\n",
    "\n",
    "            \n",
    "            # wait until all old rows are detached\n",
    "            #await page.wait_for_selector(\"li.usview\", state=\"detached\", timeout=10000)\n",
    "            # then wait for a fresh set of hits to appear\n",
    "            #await page.wait_for_selector(\"li.usview\", timeout=20000)\n",
    "\n",
    "        await browser.close()\n",
    "    return all_texts\n",
    "\n",
    "# In your Jupyter cell:\n",
    "docs = await scrape_energy_all_pages()\n",
    "print(f\"üèÅ Scraped {len(docs)} documents total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba83c8e-5d27-4c73-a7e5-70b9fe07231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a flat list:\n",
    "H = list(set(docs_actual))\n",
    "df = pd.DataFrame(H, columns=[\"Document Text\"])\n",
    "df.to_csv(\"colorado_energy_data_unfiltered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d478f5f6-c8ea-4a5c-9296-5130940d1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('colorado_energy_data_unfiltered.csv')\n",
    "df['year'] = df.apply(lambda row: extract_history_marker(row['Document Text']), axis = 1)\n",
    "df = df.loc[df['year'].isnull() == False,:]\n",
    "df['Document Text'] = df.apply(lambda row: trim_to_body(row['Document Text']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c51f1dc-edd4-4dd7-a99b-3c149c59fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location'] = 'Colorado'\n",
    "df['year'] = df.apply(lambda row: 1900 + row['year'] if type(row['year']) != str and int(row['year']) < 100 else row['year'], axis = 1)\n",
    "df.to_csv(\"colorado_energy_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
