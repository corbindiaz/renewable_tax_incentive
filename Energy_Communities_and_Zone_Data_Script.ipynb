{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqN2qvQxFNBZ",
        "outputId": "d333f76b-e8ca-4f8d-bb1d-65e47c3db55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data downloaded and saved as 'FFE_Threshold_Areas.csv'\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Define the REST API endpoint\n",
        "url = \"https://arcgis.netl.doe.gov/server/rest/services/Hosted/2024_MSAs_NonMSAs_that_only_meet_the_FFE_Threshold/FeatureServer/0/query\"\n",
        "\n",
        "# Set the parameters for the query\n",
        "params = {\n",
        "    \"where\": \"1=1\",  # Retrieves all records\n",
        "    \"outFields\": \"*\",  # Retrieves all fields\n",
        "    \"f\": \"json\"  # Specifies the format as JSON\n",
        "}\n",
        "\n",
        "# Make the GET request to the API\n",
        "response = requests.get(url, params=params)\n",
        "data = response.json()\n",
        "\n",
        "# Extract the features from the response\n",
        "features = data['features']\n",
        "\n",
        "# Normalize the JSON data into a pandas DataFrame\n",
        "df = pd.json_normalize(features)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(\"FFE_Threshold_Areas.csv\", index=False)\n",
        "\n",
        "print(\"✅ Data downloaded and saved as 'FFE_Threshold_Areas.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Define the REST API endpoint\n",
        "url = \"https://arcgis.netl.doe.gov/server/rest/services/Hosted/2024_MSAs_NonMSAs_that_are_Energy_Communities/FeatureServer/0/query\"\n",
        "\n",
        "# Set the parameters for the query\n",
        "params = {\n",
        "    \"where\": \"1=1\",  # Retrieves all records\n",
        "    \"outFields\": \"*\",  # Retrieves all fields\n",
        "    \"f\": \"json\"  # Specifies the format as JSON\n",
        "}\n",
        "\n",
        "# Make the GET request to the API\n",
        "response = requests.get(url, params=params)\n",
        "data = response.json()\n",
        "\n",
        "# Extract the features from the response\n",
        "features = data['features']\n",
        "\n",
        "# Normalize the JSON data into a pandas DataFrame\n",
        "df = pd.json_normalize(features)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(\"Energy_Communities_2024.csv\", index=False)\n",
        "\n",
        "print(\"✅ Data downloaded and saved as 'Energy_Communities_2024.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODWWmn21V3ec",
        "outputId": "b601cfde-e326-4429-b603-b71389b04195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data downloaded and saved as 'Energy_Communities_2024.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create folders\n",
        "os.makedirs(\"EDX_Downloads\", exist_ok=True)\n",
        "os.makedirs(\"EDX_Extracted\", exist_ok=True)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload ZIPs manually\n",
        "\n",
        "# Move uploaded files to EDX_Downloads\n",
        "import shutil\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, f\"EDX_Downloads/{filename}\")\n",
        "\n",
        "print(\"✅ ZIPs moved to EDX_Downloads/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "4aKuXFS1YYoC",
        "outputId": "3482f06b-f5b5-45ff-e56d-b69d41f13ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-414652e3-ca38-419a-9bb8-a61d8831291b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-414652e3-ca38-419a-9bb8-a61d8831291b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ira_coal_closure_energy_comm_2023v2.zip to ira_coal_closure_energy_comm_2023v2.zip\n",
            "✅ ZIPs moved to EDX_Downloads/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create folders\n",
        "os.makedirs(\"EDX_Downloads\", exist_ok=True)\n",
        "os.makedirs(\"EDX_Extracted\", exist_ok=True)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload ZIPs manually\n",
        "\n",
        "# Move uploaded files to EDX_Downloads\n",
        "import shutil\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, f\"EDX_Downloads/{filename}\")\n",
        "\n",
        "print(\"✅ ZIPs moved to EDX_Downloads/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "QFSEIQ48vbkc",
        "outputId": "101b124f-7995-4bd2-b7f2-f8c938a0bc86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fadcb51d-2d48-4b21-8700-b6194765463f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fadcb51d-2d48-4b21-8700-b6194765463f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving MSA_NMSA_EC_FFE_Status_v2023_3.zip to MSA_NMSA_EC_FFE_Status_v2023_3.zip\n",
            "✅ ZIPs moved to EDX_Downloads/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create folders\n",
        "os.makedirs(\"EDX_Downloads\", exist_ok=True)\n",
        "os.makedirs(\"EDX_Extracted\", exist_ok=True)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload ZIPs manually\n",
        "\n",
        "# Move uploaded files to EDX_Downloads\n",
        "import shutil\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, f\"EDX_Downloads/{filename}\")\n",
        "\n",
        "print(\"✅ ZIPs moved to EDX_Downloads/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "BeoMuPzhvel7",
        "outputId": "1d3357f8-c290-4bda-a753-a4c1bab224f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8b713a4a-2485-4661-9524-90beddb2b499\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8b713a4a-2485-4661-9524-90beddb2b499\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving MSA_NMSA_EC_FFE_v2024_1.zip to MSA_NMSA_EC_FFE_v2024_1.zip\n",
            "✅ ZIPs moved to EDX_Downloads/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create folders\n",
        "os.makedirs(\"EDX_Downloads\", exist_ok=True)\n",
        "os.makedirs(\"EDX_Extracted\", exist_ok=True)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload ZIPs manually\n",
        "\n",
        "# Move uploaded files to EDX_Downloads\n",
        "import shutil\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, f\"EDX_Downloads/{filename}\")\n",
        "\n",
        "print(\"✅ ZIPs moved to EDX_Downloads/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "t8Kd_G0Uvjig",
        "outputId": "4270c5bc-e741-4849-aca7-05c18b7befe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cedfc91e-aa7d-426a-9b38-4136179e0afe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cedfc91e-aa7d-426a-9b38-4136179e0afe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Coal_Closures_EnergyComm_v2024_1.zip to Coal_Closures_EnergyComm_v2024_1.zip\n",
            "✅ ZIPs moved to EDX_Downloads/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create folders\n",
        "os.makedirs(\"48C_Downloads\", exist_ok=True)\n",
        "os.makedirs(\"48C_Extracted\", exist_ok=True)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload ZIPs manually\n",
        "\n",
        "# Move uploaded files to EDX_Downloads\n",
        "import shutil\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, f\"48C_Downloads/{filename}\")\n",
        "\n",
        "print(\"✅ ZIPs moved to 48C_Downloads/\")\n"
      ],
      "metadata": {
        "id": "WY9tQIaZk9Hp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "fcc0bd5d-4950-4077-b078-72ae5a70f443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-78984182-0a2e-4720-95a8-9c4dcfe3f482\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-78984182-0a2e-4720-95a8-9c4dcfe3f482\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 48c_data.zip to 48c_data.zip\n",
            "✅ ZIPs moved to 48C_Downloads/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "# Set folders\n",
        "download_dir = \"48C_Downloads\"\n",
        "extract_dir = \"48C_Extracted\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "csv_paths = []\n",
        "xlsx_paths = []\n",
        "\n",
        "# Step 1: Extract ZIPs and collect CSV/XLSX paths\n",
        "for file in os.listdir(download_dir):\n",
        "    if file.lower().endswith(\".zip\"):\n",
        "        zip_path = os.path.join(download_dir, file)\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_dir)\n",
        "                print(f\"📦 Extracted: {file}\")\n",
        "                for name in zip_ref.namelist():\n",
        "                    lower = name.lower()\n",
        "                    full_path = os.path.join(extract_dir, name)\n",
        "                    if lower.endswith(\".csv\"):\n",
        "                        csv_paths.append(full_path)\n",
        "                    elif lower.endswith(\".xlsx\"):\n",
        "                        xlsx_paths.append(full_path)\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"❌ Not a valid ZIP file: {file}\")\n",
        "\n",
        "# Step 2: Load CSVs with encoding fallback\n",
        "dfs = []\n",
        "\n",
        "for path in csv_paths:\n",
        "    try:\n",
        "        df = pd.read_csv(path, encoding=\"utf-8\")\n",
        "    except UnicodeDecodeError:\n",
        "        df = pd.read_csv(path, encoding=\"ISO-8859-1\")\n",
        "    df['source_file'] = os.path.basename(path)\n",
        "    dfs.append(df)\n",
        "    print(f\"✅ Loaded CSV: {path}\")\n",
        "\n",
        "# Step 3: Load XLSX files\n",
        "for path in xlsx_paths:\n",
        "    try:\n",
        "        df = pd.read_excel(path)  # You can add sheet_name='Sheet1' if needed\n",
        "        df['source_file'] = os.path.basename(path)\n",
        "        dfs.append(df)\n",
        "        print(f\"✅ Loaded XLSX: {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Failed to read {path}: {e}\")\n",
        "\n",
        "# Step 4: Merge and export\n",
        "if dfs:\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "    combined_df.to_csv(\"48C_Combined_Data.csv\", index=False)\n",
        "    print(\"✅ Merged CSV/XLSX saved as '48C_Combined_Data.csv'\")\n",
        "else:\n",
        "    print(\"❌ No CSV or XLSX files found to merge.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80vxul_AlMr9",
        "outputId": "ef70d6f0-a93c-4ccc-ee08-974e40113dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Extracted: 48c_data.zip\n",
            "✅ Loaded CSV: 48C_Extracted/48C_data/48C_CensusTractDesignation.csv\n",
            "✅ Loaded XLSX: 48C_Extracted/48C_data/EnergyCommunities_48C_CensusTractDesignation.xlsx\n",
            "✅ Merged CSV/XLSX saved as '48C_Combined_Data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "# Set folders\n",
        "download_dir = \"EDX_Downloads\"\n",
        "extract_dir = \"EDX_Extracted\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "csv_paths = []\n",
        "xlsx_paths = []\n",
        "\n",
        "# Step 1: Extract ZIPs and collect CSV/XLSX paths\n",
        "for file in os.listdir(download_dir):\n",
        "    if file.lower().endswith(\".zip\"):\n",
        "        zip_path = os.path.join(download_dir, file)\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_dir)\n",
        "                print(f\"📦 Extracted: {file}\")\n",
        "                for name in zip_ref.namelist():\n",
        "                    lower = name.lower()\n",
        "                    full_path = os.path.join(extract_dir, name)\n",
        "                    if lower.endswith(\".csv\"):\n",
        "                        csv_paths.append(full_path)\n",
        "                    elif lower.endswith(\".xlsx\"):\n",
        "                        xlsx_paths.append(full_path)\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"❌ Not a valid ZIP file: {file}\")\n",
        "\n",
        "# Step 2: Load CSVs with encoding fallback\n",
        "dfs = []\n",
        "\n",
        "for path in csv_paths:\n",
        "    try:\n",
        "        df = pd.read_csv(path, encoding=\"utf-8\")\n",
        "    except UnicodeDecodeError:\n",
        "        df = pd.read_csv(path, encoding=\"ISO-8859-1\")\n",
        "    df['source_file'] = os.path.basename(path)\n",
        "    dfs.append(df)\n",
        "    print(f\"✅ Loaded CSV: {path}\")\n",
        "\n",
        "# Step 3: Load XLSX files\n",
        "for path in xlsx_paths:\n",
        "    try:\n",
        "        df = pd.read_excel(path)  # You can add sheet_name='Sheet1' if needed\n",
        "        df['source_file'] = os.path.basename(path)\n",
        "        dfs.append(df)\n",
        "        print(f\"✅ Loaded XLSX: {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Failed to read {path}: {e}\")\n",
        "\n",
        "# Step 4: Merge and export\n",
        "if dfs:\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "    combined_df.to_csv(\"EDX_Combined_Data.csv\", index=False)\n",
        "    print(\"✅ Merged CSV/XLSX saved as 'EDX_Combined_Data.csv'\")\n",
        "else:\n",
        "    print(\"❌ No CSV or XLSX files found to merge.\")\n"
      ],
      "metadata": {
        "id": "d-aQ0CSEPEKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76d0404-f098-4e3c-818b-e2a7468f14ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Extracted: Coal_Closures_EnergyComm_v2024_1.zip\n",
            "📦 Extracted: ira_coal_closure_energy_comm_2023v2.zip\n",
            "📦 Extracted: MSA_NMSA_EC_FFE_v2024_1.zip\n",
            "📦 Extracted: MSA_NMSA_EC_FFE_Status_v2023_3.zip\n",
            "✅ Loaded CSV: EDX_Extracted/Coal_Closures_EnergyComm_v2024_1/IRA_EnergyComm_CTracts_CoalClosures_v2024_1.csv\n",
            "✅ Loaded CSV: EDX_Extracted/IRA_Coal_Closure_Energy_Comm_2023v2/Coal_Closure_Energy_Communities_2023v2.csv\n",
            "✅ Loaded CSV: EDX_Extracted/MSA_NMSA_EC_FFE_v2024_1/MSA_NonMSA_EnergyCommunities_FossilFuelEmp_v2024_1.csv\n",
            "✅ Loaded CSV: EDX_Extracted/MSA_NMSA_EC_FFE_Status_v2023_3/MSA_NonMSA_EnergyCommunities_FossilFuelEmp_2023v3.csv\n",
            "✅ Loaded XLSX: EDX_Extracted/Coal_Closures_EnergyComm_v2024_1/IRA_EnergyComm_CTracts_CoalClosures_v2024_1.xlsx\n",
            "✅ Loaded XLSX: EDX_Extracted/IRA_Coal_Closure_Energy_Comm_2023v2/IRA_EnergyCommunities_CensusTracts_CoalClosures_2023v2.xlsx\n",
            "✅ Loaded XLSX: EDX_Extracted/MSA_NMSA_EC_FFE_v2024_1/MSA_NonMSA_EnergyCommunities_FossilFuelEmp_v2024_1.xlsx\n",
            "✅ Loaded XLSX: EDX_Extracted/MSA_NMSA_EC_FFE_Status_v2023_3/MSA_NonMSA_EnergyCommunities_FossilFuelEmp_2023v3.xlsx\n",
            "✅ Merged CSV/XLSX saved as 'EDX_Combined_Data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# ArcGIS REST API endpoint\n",
        "url = \"https://arcgis.netl.doe.gov/server/rest/services/Hosted/US_Power_Plants/FeatureServer/0/query\"\n",
        "\n",
        "# Parameters for the request\n",
        "params = {\n",
        "    \"where\": \"LOWER(plant_status) = 'closed'\",\n",
        "    \"outFields\": \"*\",  # Get all available fields\n",
        "    \"f\": \"json\",\n",
        "    \"resultRecordCount\": 8000,\n",
        "    \"orderByFields\": \"objectid\"\n",
        "}\n",
        "\n",
        "# Send request\n",
        "response = requests.get(url, params=params)\n",
        "data = response.json()\n",
        "\n",
        "# Extract features safely\n",
        "features = data.get(\"features\", [])\n",
        "\n",
        "if not features:\n",
        "    print(\"❌ No data found.\")\n",
        "else:\n",
        "    # Each feature should have an 'attributes' dictionary\n",
        "    records = [f[\"attributes\"] for f in features if \"attributes\" in f]\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(\"Closed_US_Power_Plants.csv\", index=False)\n",
        "    print(\"✅ Data saved as 'Closed_US_Power_Plants.csv'\")\n"
      ],
      "metadata": {
        "id": "pbFh2J2rTKmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9c7ca1-4458-4543-f66d-997523e515dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data saved as 'Closed_US_Power_Plants.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Define the REST API endpoint\n",
        "url = \"https://arcgis.netl.doe.gov/server/rest/services/Hosted/ManFacNAICS3133_Emissions/FeatureServer/0/query\"\n",
        "\n",
        "# Set the query parameters (JSON format, not PBF)\n",
        "params = {\n",
        "    \"where\": \"LOWER(status_48c) = 'eligible for 48c tax credit as a designated energy community'\",\n",
        "    \"outFields\": \"naicc_ss_desc,objectid_12\",\n",
        "    \"orderByFields\": \"objectid_12\",\n",
        "    \"f\": \"json\",\n",
        "    \"resultRecordCount\": 8000\n",
        "}\n",
        "\n",
        "# Send GET request\n",
        "response = requests.get(url, params=params)\n",
        "data = response.json()\n",
        "\n",
        "# Extract features safely\n",
        "features = data.get(\"features\", [])\n",
        "if not features:\n",
        "    print(\"❌ No matching records found.\")\n",
        "else:\n",
        "    records = [f[\"attributes\"] for f in features if \"attributes\" in f]\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(\"Eligible_48C_Manufacturing_Facilities.csv\", index=False)\n",
        "    print(\"✅ Saved as 'Eligible_48C_Manufacturing_Facilities.csv'\")\n"
      ],
      "metadata": {
        "id": "QVVQzO9nTZKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "837ba5e7-5c0a-4c73-b5f7-dfa30e245ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved as 'Eligible_48C_Manufacturing_Facilities.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Base endpoint and layer\n",
        "layer_url = \"https://arcgis.netl.doe.gov/server/rest/services/Hosted/Eligiblefor48C_EC_April2024Update/FeatureServer/72\"\n",
        "\n",
        "# Step 1: Get all object IDs\n",
        "oid_url = f\"{layer_url}/query\"\n",
        "oid_params = {\n",
        "    \"f\": \"json\",\n",
        "    \"returnIdsOnly\": \"true\",\n",
        "    \"where\": \"1=1\"\n",
        "}\n",
        "\n",
        "oid_resp = requests.get(oid_url, params=oid_params)\n",
        "oid_data = oid_resp.json()\n",
        "object_ids = oid_data.get(\"objectIds\", [])\n",
        "\n",
        "print(f\"🔢 Found {len(object_ids)} object IDs.\")\n",
        "\n",
        "# Step 2: Query in batches (ArcGIS has limits on URL length / record count)\n",
        "all_features = []\n",
        "batch_size = 100  # adjust if needed\n",
        "\n",
        "for i in range(0, len(object_ids), batch_size):\n",
        "    batch_ids = object_ids[i:i + batch_size]\n",
        "    query_params = {\n",
        "        \"f\": \"json\",\n",
        "        \"objectIds\": \",\".join(map(str, batch_ids)),\n",
        "        \"outFields\": \"*\",\n",
        "        \"spatialRel\": \"esriSpatialRelIntersects\"\n",
        "    }\n",
        "\n",
        "    query_resp = requests.get(f\"{layer_url}/query\", params=query_params)\n",
        "    query_data = query_resp.json()\n",
        "    features = query_data.get(\"features\", [])\n",
        "\n",
        "    if features:\n",
        "        all_features.extend([f[\"attributes\"] for f in features])\n",
        "        print(f\"✅ Retrieved {len(features)} records from batch {i // batch_size + 1}\")\n",
        "    else:\n",
        "        print(f\"⚠️ No features found in batch {i // batch_size + 1}\")\n",
        "\n",
        "    time.sleep(0.2)  # small delay to be polite to the server\n",
        "\n",
        "# Step 3: Convert to DataFrame\n",
        "if all_features:\n",
        "    df = pd.DataFrame(all_features)\n",
        "    df.to_csv(\"Eligiblefor48C_EC_Layer72.csv\", index=False)\n",
        "    print(\"📦 Data saved as 'Eligiblefor48C_EC_Layer72.csv'\")\n",
        "else:\n",
        "    print(\"❌ No data collected.\")\n"
      ],
      "metadata": {
        "id": "iSjxY6W3UB_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ac8d02-cc7c-4cf2-f2f4-f02ba065c5fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔢 Found 4318 object IDs.\n",
            "✅ Retrieved 100 records from batch 1\n",
            "✅ Retrieved 100 records from batch 2\n",
            "✅ Retrieved 100 records from batch 3\n",
            "✅ Retrieved 100 records from batch 4\n",
            "✅ Retrieved 100 records from batch 5\n",
            "✅ Retrieved 100 records from batch 6\n",
            "✅ Retrieved 100 records from batch 7\n",
            "✅ Retrieved 100 records from batch 8\n",
            "✅ Retrieved 100 records from batch 9\n",
            "✅ Retrieved 100 records from batch 10\n",
            "✅ Retrieved 100 records from batch 11\n",
            "✅ Retrieved 100 records from batch 12\n",
            "✅ Retrieved 100 records from batch 13\n",
            "✅ Retrieved 100 records from batch 14\n",
            "✅ Retrieved 100 records from batch 15\n",
            "✅ Retrieved 100 records from batch 16\n",
            "✅ Retrieved 100 records from batch 17\n",
            "✅ Retrieved 100 records from batch 18\n",
            "✅ Retrieved 100 records from batch 19\n",
            "✅ Retrieved 100 records from batch 20\n",
            "✅ Retrieved 100 records from batch 21\n",
            "✅ Retrieved 100 records from batch 22\n",
            "✅ Retrieved 100 records from batch 23\n",
            "✅ Retrieved 100 records from batch 24\n",
            "✅ Retrieved 100 records from batch 25\n",
            "✅ Retrieved 100 records from batch 26\n",
            "✅ Retrieved 100 records from batch 27\n",
            "✅ Retrieved 100 records from batch 28\n",
            "✅ Retrieved 100 records from batch 29\n",
            "✅ Retrieved 100 records from batch 30\n",
            "✅ Retrieved 100 records from batch 31\n",
            "✅ Retrieved 100 records from batch 32\n",
            "✅ Retrieved 100 records from batch 33\n",
            "✅ Retrieved 100 records from batch 34\n",
            "✅ Retrieved 100 records from batch 35\n",
            "✅ Retrieved 100 records from batch 36\n",
            "✅ Retrieved 100 records from batch 37\n",
            "✅ Retrieved 100 records from batch 38\n",
            "✅ Retrieved 100 records from batch 39\n",
            "✅ Retrieved 100 records from batch 40\n",
            "✅ Retrieved 100 records from batch 41\n",
            "✅ Retrieved 100 records from batch 42\n",
            "✅ Retrieved 100 records from batch 43\n",
            "✅ Retrieved 18 records from batch 44\n",
            "📦 Data saved as 'Eligiblefor48C_EC_Layer72.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Define the REST endpoint for Layer 73\n",
        "url = \"https://arcgis.netl.doe.gov/server/rest/services/Hosted/Eligiblefor48C_April2024Update/FeatureServer/73/query\"\n",
        "\n",
        "# Define the query parameters\n",
        "params = {\n",
        "    \"where\": \"1=1\",              # No filter — get all records\n",
        "    \"outFields\": \"*\",            # Get all fields\n",
        "    \"f\": \"json\",                 # Get output in JSON format\n",
        "    \"returnGeometry\": False      # Optional: skip geometry to make it simpler\n",
        "}\n",
        "\n",
        "# Make the request\n",
        "response = requests.get(url, params=params)\n",
        "data = response.json()\n",
        "\n",
        "# Extract attributes from features\n",
        "features = data.get(\"features\", [])\n",
        "records = [f[\"attributes\"] for f in features]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# Preview or save\n",
        "print(df.head())\n",
        "df.to_csv(\"Layer73_Eligible_48C.csv\", index=False)\n",
        "print(\"✅ Data saved as 'Layer73_Eligible_48C.csv'\")\n"
      ],
      "metadata": {
        "id": "JnjLPKSGXtj5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bccc670-ae29-4f34-ccaf-ae3819d87a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tract_fip  f48c_status_bin  date_last_update county_fip  \\\n",
            "0    020100              0.0     1714348800000        001   \n",
            "1    020200              0.0     1714348800000        001   \n",
            "2    020300              0.0     1714348800000        001   \n",
            "3    020801              0.0     1714348800000        001   \n",
            "4    020803              0.0     1714348800000        001   \n",
            "\n",
            "                                               label     county_name  \\\n",
            "0  is eligible for a 48C tax credit but is NOT el...  Autauga County   \n",
            "1  is eligible for a 48C tax credit but is NOT el...  Autauga County   \n",
            "2  is eligible for a 48C tax credit but is NOT el...  Autauga County   \n",
            "3  is eligible for a 48C tax credit but is NOT el...  Autauga County   \n",
            "4  is eligible for a 48C tax credit but is NOT el...  Autauga County   \n",
            "\n",
            "            tract_name state_fip  date_record_added state_name  \\\n",
            "0     Census Tract 201        01      1684972800000    Alabama   \n",
            "1     Census Tract 202        01      1684972800000    Alabama   \n",
            "2     Census Tract 203        01      1684972800000    Alabama   \n",
            "3  Census Tract 208.01        01      1684972800000    Alabama   \n",
            "4  Census Tract 208.03        01      1684972800000    Alabama   \n",
            "\n",
            "  ctract_geoid_2020 f48c_tract_status  dataset_version  SHAPE__Length  \\\n",
            "0       01001020100               N/A                2   19225.805119   \n",
            "1       01001020200               N/A                2   11645.799063   \n",
            "2       01001020300               N/A                2   12474.648706   \n",
            "3       01001020801               N/A                2   72259.359522   \n",
            "4       01001020803               N/A                2   61631.543152   \n",
            "\n",
            "   objectid   SHAPE__Area  \n",
            "0         1  1.388550e+07  \n",
            "1         2  4.687256e+06  \n",
            "2         3  7.550452e+06  \n",
            "3        10  1.862978e+08  \n",
            "4        11  1.828805e+08  \n",
            "✅ Data saved as 'Layer73_Eligible_48C.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Define the REST API endpoint for Layer 23\n",
        "url = \"https://arcgis.netl.doe.gov/server/rest/services/Hosted/Eligible_for_48C_tax_credit_but_NOT_as_a_designated_energy_community/FeatureServer/23/query\"\n",
        "\n",
        "# Query parameters\n",
        "params = {\n",
        "    \"where\": \"1=1\",           # Get all records\n",
        "    \"outFields\": \"*\",         # Return all fields\n",
        "    \"returnGeometry\": False,  # Skip geometry\n",
        "    \"f\": \"json\"               # Return as JSON\n",
        "}\n",
        "\n",
        "# Make the GET request\n",
        "response = requests.get(url, params=params)\n",
        "data = response.json()\n",
        "\n",
        "# Extract attributes\n",
        "features = data.get(\"features\", [])\n",
        "records = [f[\"attributes\"] for f in features]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(\"Layer23_Eligible_48C_Not_Designated_EC.csv\", index=False)\n",
        "print(\"✅ Saved as 'Layer23_Eligible_48C_Not_Designated_EC.csv'\")\n"
      ],
      "metadata": {
        "id": "fMd-L6fTYP2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9e55b0-538a-444d-a263-32c5c2eaeb19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved as 'Layer23_Eligible_48C_Not_Designated_EC.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# REST API endpoint\n",
        "url = \"https://arcgis.netl.doe.gov/server/rest/services/Hosted/US_generators_coal/FeatureServer/0/query\"\n",
        "\n",
        "# Define parameters to return full data\n",
        "params = {\n",
        "    \"where\": \"((f_860m_retirementyear <= 2030) AND (f_860m_nameplatecapacity_mw_ >= 53) AND (UPPER(f_860m_operationalstatus_aug_05) IN ('RETIRED','CANCELEDORPOSTPONED','OPERABLE')) AND (solar_mean >= 1947) AND (forest_land <= 18) AND (elev_var_norm <= 0.09))\",\n",
        "    \"outFields\": \"*\",\n",
        "    \"returnGeometry\": False,\n",
        "    \"f\": \"json\"\n",
        "}\n",
        "\n",
        "# Make the GET request\n",
        "response = requests.get(url, params=params)\n",
        "data = response.json()\n",
        "\n",
        "# Extract attributes\n",
        "features = data.get(\"features\", [])\n",
        "records = [f[\"attributes\"] for f in features]\n",
        "\n",
        "# Convert to DataFrame and save\n",
        "df = pd.DataFrame(records)\n",
        "df.to_csv(\"Filtered_US_Coal_Generators.csv\", index=False)\n",
        "print(\"✅ Saved as 'Filtered_US_Coal_Generators.csv'\")\n"
      ],
      "metadata": {
        "id": "tv5ZaQKXZaWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3044603-b049-4a76-ee58-f0b9d807fd6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved as 'Filtered_US_Coal_Generators.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Base URL for the layer\n",
        "base_url = \"https://arcgis.netl.doe.gov/server/rest/services/Hosted/US_generators_coal/FeatureServer/0/query\"\n",
        "\n",
        "# Parameters common to each request\n",
        "base_params = {\n",
        "    \"f\": \"json\",  # use 'json' for easy parsing\n",
        "    \"where\": \"1=1\",\n",
        "    \"outFields\": \"*\",\n",
        "    \"returnGeometry\": False,\n",
        "    \"spatialRel\": \"esriSpatialRelIntersects\",\n",
        "    \"orderByFields\": \"objectid ASC\",  # ensure consistent paging\n",
        "}\n",
        "\n",
        "all_records = []\n",
        "offset = 0\n",
        "page_size = 1000  # You can adjust this based on server limits\n",
        "\n",
        "while True:\n",
        "    print(f\"🔄 Fetching records {offset} to {offset + page_size - 1}\")\n",
        "\n",
        "    # Merge base parameters with paging\n",
        "    params = {**base_params, \"resultOffset\": offset, \"resultRecordCount\": page_size}\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    features = data.get(\"features\", [])\n",
        "    if not features:\n",
        "        break  # Exit when no more data\n",
        "\n",
        "    records = [f[\"attributes\"] for f in features]\n",
        "    all_records.extend(records)\n",
        "\n",
        "    offset += page_size\n",
        "    time.sleep(0.2)  # be kind to the server\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(all_records)\n",
        "df.to_csv(\"All_US_Coal_Generators.csv\", index=False)\n",
        "print(f\"✅ Download complete. Total records: {len(df)}\")\n"
      ],
      "metadata": {
        "id": "VOPuMGD3ZcWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb19540c-76a5-4f0b-e5ed-7a7586fdedb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Fetching records 0 to 999\n",
            "🔄 Fetching records 1000 to 1999\n",
            "🔄 Fetching records 2000 to 2999\n",
            "✅ Download complete. Total records: 1401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"All_US_Coal_Generators.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "N0P9h9mEbHVG",
        "outputId": "8674bc81-15f0-4ab3-beee-2817897c1a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_24ab411c-b9ea-4423-b44c-71061571cc3d\", \"All_US_Coal_Generators.csv\", 1059930)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Target URL\n",
        "url = \"https://data.nrel.gov/submissions/238\"\n",
        "\n",
        "# Create download folder\n",
        "download_folder = \"nrel_steady_solar_scraped\"\n",
        "os.makedirs(download_folder, exist_ok=True)\n",
        "\n",
        "# Scrape page\n",
        "print(f\"🌐 Scraping {url}...\")\n",
        "response = requests.get(url)\n",
        "if response.status_code != 200:\n",
        "    raise Exception(f\"Failed to fetch page: {response.status_code}\")\n",
        "\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Find all download links\n",
        "download_links = []\n",
        "for a_tag in soup.find_all(\"a\", href=True):\n",
        "    href = a_tag[\"href\"]\n",
        "    if any(href.lower().endswith(ext) for ext in [\".csv\", \".xlsx\", \".zip\", \".pdf\"]):\n",
        "        full_link = href\n",
        "        if not full_link.startswith(\"http\"):\n",
        "            full_link = \"https://data.nrel.gov\" + full_link\n",
        "        download_links.append(full_link)\n",
        "\n",
        "# Download each file\n",
        "for link in download_links:\n",
        "    filename = os.path.basename(link.split(\"?\")[0])\n",
        "    dest_path = os.path.join(download_folder, filename)\n",
        "\n",
        "    print(f\"⬇️ Downloading {filename}...\")\n",
        "    file_response = requests.get(link)\n",
        "    if file_response.status_code == 200:\n",
        "        with open(dest_path, \"wb\") as f:\n",
        "            f.write(file_response.content)\n",
        "        print(f\"✅ Saved to {dest_path}\")\n",
        "    else:\n",
        "        print(f\"❌ Failed to download {filename}. Status code: {file_response.status_code}\")\n",
        "\n",
        "print(\"\\n🎉 All files downloaded successfully!\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Now, load files into DataFrames\n",
        "# ----------------------------------------------------------\n",
        "dataframes = {}  # Dictionary to store all DataFrames\n",
        "\n",
        "for file in os.listdir(download_folder):\n",
        "    file_path = os.path.join(download_folder, file)\n",
        "\n",
        "    if file.lower().endswith(\".csv\"):\n",
        "        print(f\"📄 Reading CSV: {file}\")\n",
        "        df = pd.read_csv(file_path, low_memory=False)\n",
        "        dataframes[file] = df\n",
        "\n",
        "    elif file.lower().endswith(\".xlsx\"):\n",
        "        print(f\"📄 Reading Excel: {file}\")\n",
        "        df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
        "        dataframes[file] = df\n",
        "\n",
        "    elif file.lower().endswith(\".zip\"):\n",
        "        print(f\"📦 Extracting ZIP: {file}\")\n",
        "        extract_folder = os.path.join(download_folder, file.replace(\".zip\", \"\"))\n",
        "        os.makedirs(extract_folder, exist_ok=True)\n",
        "        with ZipFile(file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_folder)\n",
        "        print(f\"✅ Extracted to {extract_folder}\")\n",
        "\n",
        "print(\"\\n✅ All data files loaded and ready!\")\n",
        "\n",
        "# Optional: Check what we loaded\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"\\n🗂️ {name} - {df.shape[0]} rows x {df.shape[1]} columns\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU_wPPeP8ICN",
        "outputId": "ffe95366-3461-4a9e-ff0d-d26a73c600ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 Scraping https://data.nrel.gov/submissions/238...\n",
            "⬇️ Downloading 85722.pdf...\n",
            "✅ Saved to nrel_steady_solar_scraped/85722.pdf\n",
            "⬇️ Downloading 1716529424-STEADy_May2024_3.xlsx...\n",
            "✅ Saved to nrel_steady_solar_scraped/1716529424-STEADy_May2024_3.xlsx\n",
            "⬇️ Downloading 1716529424-STEADy_May2024_4.zip...\n",
            "✅ Saved to nrel_steady_solar_scraped/1716529424-STEADy_May2024_4.zip\n",
            "⬇️ Downloading 1716529424-STEADy_May2024_5.csv...\n",
            "✅ Saved to nrel_steady_solar_scraped/1716529424-STEADy_May2024_5.csv\n",
            "⬇️ Downloading 85722.pdf...\n",
            "✅ Saved to nrel_steady_solar_scraped/85722.pdf\n",
            "⬇️ Downloading 1716529424-STEADy_May2024_3.xlsx...\n",
            "✅ Saved to nrel_steady_solar_scraped/1716529424-STEADy_May2024_3.xlsx\n",
            "⬇️ Downloading 1716529424-STEADy_May2024_4.zip...\n",
            "✅ Saved to nrel_steady_solar_scraped/1716529424-STEADy_May2024_4.zip\n",
            "⬇️ Downloading 1716529424-STEADy_May2024_5.csv...\n",
            "✅ Saved to nrel_steady_solar_scraped/1716529424-STEADy_May2024_5.csv\n",
            "\n",
            "🎉 All files downloaded successfully!\n",
            "📄 Reading CSV: 1716529424-STEADy_May2024_5.csv\n",
            "📦 Extracting ZIP: 1716529424-STEADy_May2024_4.zip\n",
            "✅ Extracted to nrel_steady_solar_scraped/1716529424-STEADy_May2024_4\n",
            "📄 Reading Excel: 1716529424-STEADy_May2024_3.xlsx\n",
            "\n",
            "✅ All data files loaded and ready!\n",
            "\n",
            "🗂️ 1716529424-STEADy_May2024_5.csv - 85160 rows x 50 columns\n",
            "\n",
            "🗂️ 1716529424-STEADy_May2024_3.xlsx - 49 rows x 5 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# =============================\n",
        "# Step 1: Load manual CSV files\n",
        "# =============================\n",
        "\n",
        "manual_files = {\n",
        "    \"edx_combined_data\": \"EDX_Combined_Data.csv\",\n",
        "    \"closed_us_power_plants\": \"Closed_US_Power_Plants.csv\",\n",
        "    \"eligible_48c_mfg_facilities\": \"Eligible_48C_Manufacturing_Facilities.csv\",\n",
        "    \"layer73_eligible_48c\": \"Layer73_Eligible_48C.csv\",\n",
        "    \"layer23_eligible_48c_not_ec\": \"Layer23_Eligible_48C_Not_Designated_EC.csv\",\n",
        "    \"filtered_us_coal_generators\": \"Filtered_US_Coal_Generators.csv\",\n",
        "    \"all_us_coal_generators\": \"All_US_Coal_Generators.csv\",\n",
        "    \"ffe_threshold_areas_csv\": \"FFE_Threshold_Areas.csv\",\n",
        "    \"energy_communities_2024_csv\": \"Energy_Communities_2024.csv\",\n",
        "    \"combined_48c_data_csv\": \"48C_Combined_Data.csv\",\n",
        "}\n",
        "\n",
        "all_dataframes = {}\n",
        "\n",
        "for name, filepath in manual_files.items():\n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"📥 Loading {filepath}...\")\n",
        "        all_dataframes[name] = pd.read_csv(filepath, low_memory=False)\n",
        "    else:\n",
        "        print(f\"⚠️ File not found: {filepath}\")\n",
        "\n",
        "# =============================\n",
        "# Step 2: Scrape NREL 238 page\n",
        "# =============================\n",
        "\n",
        "nrel_folder = \"nrel_steady_solar_scraped\"\n",
        "os.makedirs(nrel_folder, exist_ok=True)\n",
        "\n",
        "nrel_url = \"https://data.nrel.gov/submissions/238\"\n",
        "\n",
        "print(f\"🌐 Scraping {nrel_url}...\")\n",
        "r = requests.get(nrel_url)\n",
        "soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "# Find and download all links\n",
        "scraped_dataframes = {}\n",
        "\n",
        "for a_tag in soup.find_all(\"a\", href=True):\n",
        "    href = a_tag['href']\n",
        "    if any(href.lower().endswith(ext) for ext in [\".csv\", \".xlsx\", \".zip\", \".pdf\"]):\n",
        "        full_link = href\n",
        "        if not full_link.startswith(\"http\"):\n",
        "            full_link = \"https://data.nrel.gov\" + href\n",
        "\n",
        "        filename = os.path.basename(full_link.split(\"?\")[0])\n",
        "        dest_path = os.path.join(nrel_folder, filename)\n",
        "\n",
        "        print(f\"⬇️ Downloading {filename}...\")\n",
        "        r_file = requests.get(full_link)\n",
        "        if r_file.status_code == 200:\n",
        "            with open(dest_path, \"wb\") as f:\n",
        "                f.write(r_file.content)\n",
        "            print(f\"✅ Saved {filename}\")\n",
        "\n",
        "            # Load if CSV/Excel\n",
        "            if filename.lower().endswith(\".csv\"):\n",
        "                scraped_dataframes[filename] = pd.read_csv(dest_path, low_memory=False)\n",
        "            elif filename.lower().endswith(\".xlsx\"):\n",
        "                scraped_dataframes[filename] = pd.read_excel(dest_path, engine=\"openpyxl\")\n",
        "            elif filename.lower().endswith(\".zip\"):\n",
        "                with ZipFile(dest_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(nrel_folder)\n",
        "                print(f\"📦 Extracted {filename}\")\n",
        "\n",
        "print(\"\\n✅ Finished scraping and loading NREL files!\")\n",
        "\n",
        "# Merge scraped into all_dataframes\n",
        "all_dataframes.update(scraped_dataframes)\n",
        "\n",
        "# =============================\n",
        "# Step 3: Smarter Year/State Extraction\n",
        "# =============================\n",
        "\n",
        "# Full State Mapping\n",
        "state_abbrev_to_name = {\n",
        "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
        "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
        "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
        "    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
        "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n",
        "    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n",
        "    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n",
        "    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
        "    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n",
        "    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n",
        "    'DC': 'District of Columbia'\n",
        "}\n",
        "\n",
        "def smart_find_year(row):\n",
        "    for col in row.index:\n",
        "        if \"year\" in col.lower():\n",
        "            return row[col]\n",
        "        if \"date\" in col.lower() and pd.notna(row[col]):\n",
        "            try:\n",
        "                return pd.to_datetime(row[col]).year\n",
        "            except:\n",
        "                continue\n",
        "    return \"Unknown\"\n",
        "\n",
        "def smart_find_state(row):\n",
        "    for col in row.index:\n",
        "        if \"state\" in col.lower():\n",
        "            val = str(row[col]).strip().upper()\n",
        "            if val in state_abbrev_to_name:\n",
        "                return state_abbrev_to_name[val]\n",
        "            elif val.title() in state_abbrev_to_name.values():\n",
        "                return val.title()\n",
        "    return \"Federal\"\n",
        "\n",
        "# =============================\n",
        "# Step 4: Combine All Datasets\n",
        "# =============================\n",
        "\n",
        "final_rows = []\n",
        "\n",
        "for name, df in all_dataframes.items():\n",
        "    print(f\"\\n🔍 Processing {name}...\")\n",
        "    df = df.copy()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        year = smart_find_year(row)\n",
        "        state = smart_find_state(row)\n",
        "        row_data = row.to_dict()\n",
        "\n",
        "        final_rows.append({\n",
        "            \"source\": name,\n",
        "            \"state\": state,\n",
        "            \"year\": year,\n",
        "            \"data\": json.dumps(row_data, default=str)\n",
        "        })\n",
        "\n",
        "final_df = pd.DataFrame(final_rows)\n",
        "\n",
        "print(\"\\n✅ All datasets combined!\")\n",
        "print(final_df.head())\n",
        "\n",
        "# =============================\n",
        "# Step 5: Save Final File\n",
        "# =============================\n",
        "\n",
        "final_df.to_csv(\"all_data_combined.csv\", index=False)\n",
        "# or: final_df.to_parquet(\"all_data_combined.parquet\", index=False)\n",
        "\n",
        "print(\"\\n🎉 Final file saved as 'all_data_combined.csv'!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT5tfTBfUSkG",
        "outputId": "a4e94386-02f9-4105-e857-2fed16ddc88c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Loading EDX_Combined_Data.csv...\n",
            "📥 Loading Closed_US_Power_Plants.csv...\n",
            "📥 Loading Eligible_48C_Manufacturing_Facilities.csv...\n",
            "📥 Loading Layer73_Eligible_48C.csv...\n",
            "📥 Loading Layer23_Eligible_48C_Not_Designated_EC.csv...\n",
            "📥 Loading Filtered_US_Coal_Generators.csv...\n",
            "📥 Loading All_US_Coal_Generators.csv...\n",
            "📥 Loading FFE_Threshold_Areas.csv...\n",
            "📥 Loading Energy_Communities_2024.csv...\n",
            "📥 Loading 48C_Combined_Data.csv...\n",
            "🌐 Scraping https://data.nrel.gov/submissions/238...\n",
            "⬇️ Downloading 85722.pdf...\n",
            "✅ Saved 85722.pdf\n",
            "⬇️ Downloading 1716529424-STEADy_May2024_3.xlsx...\n",
            "✅ Saved 1716529424-STEADy_May2024_3.xlsx\n",
            "⬇️ Downloading 1716529424-STEADy_May2024_4.zip...\n",
            "✅ Saved 1716529424-STEADy_May2024_4.zip\n",
            "📦 Extracted 1716529424-STEADy_May2024_4.zip\n",
            "⬇️ Downloading 1716529424-STEADy_May2024_5.csv...\n",
            "✅ Saved 1716529424-STEADy_May2024_5.csv\n",
            "⬇️ Downloading 85722.pdf...\n",
            "✅ Saved 85722.pdf\n",
            "⬇️ Downloading 1716529424-STEADy_May2024_3.xlsx...\n",
            "✅ Saved 1716529424-STEADy_May2024_3.xlsx\n",
            "⬇️ Downloading 1716529424-STEADy_May2024_4.zip...\n",
            "✅ Saved 1716529424-STEADy_May2024_4.zip\n",
            "📦 Extracted 1716529424-STEADy_May2024_4.zip\n",
            "⬇️ Downloading 1716529424-STEADy_May2024_5.csv...\n",
            "✅ Saved 1716529424-STEADy_May2024_5.csv\n",
            "\n",
            "✅ Finished scraping and loading NREL files!\n",
            "\n",
            "🔍 Processing edx_combined_data...\n",
            "\n",
            "🔍 Processing closed_us_power_plants...\n",
            "\n",
            "🔍 Processing eligible_48c_mfg_facilities...\n",
            "\n",
            "🔍 Processing layer73_eligible_48c...\n",
            "\n",
            "🔍 Processing layer23_eligible_48c_not_ec...\n",
            "\n",
            "🔍 Processing filtered_us_coal_generators...\n",
            "\n",
            "🔍 Processing all_us_coal_generators...\n",
            "\n",
            "🔍 Processing ffe_threshold_areas_csv...\n",
            "\n",
            "🔍 Processing energy_communities_2024_csv...\n",
            "\n",
            "🔍 Processing combined_48c_data_csv...\n",
            "\n",
            "🔍 Processing 1716529424-STEADy_May2024_3.xlsx...\n",
            "\n",
            "🔍 Processing 1716529424-STEADy_May2024_5.csv...\n",
            "\n",
            "✅ All datasets combined!\n",
            "              source     state     year  \\\n",
            "0  edx_combined_data   Alabama  Unknown   \n",
            "1  edx_combined_data  Colorado  Unknown   \n",
            "2  edx_combined_data  Colorado  Unknown   \n",
            "3  edx_combined_data   Indiana  Unknown   \n",
            "4  edx_combined_data  Kentucky  Unknown   \n",
            "\n",
            "                                                data  \n",
            "0  {\"AFFGEOID_Tract_2020\": \"1400000US01049960200\"...  \n",
            "1  {\"AFFGEOID_Tract_2020\": \"1400000US08077001900\"...  \n",
            "2  {\"AFFGEOID_Tract_2020\": \"1400000US08085966100\"...  \n",
            "3  {\"AFFGEOID_Tract_2020\": \"1400000US18129040400\"...  \n",
            "4  {\"AFFGEOID_Tract_2020\": \"1400000US21013960500\"...  \n",
            "\n",
            "🎉 Final file saved as 'all_data_combined.csv'!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"all_data_combined.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "K2gKy8AiUY7K",
        "outputId": "9ed17f99-67f1-452e-da9f-aa5fb8827895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_928089ee-370d-4a2a-8f02-d9a0908708fb\", \"all_data_combined.csv\", 458399771)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# --- 1. Load the manual CSV files you have\n",
        "manual_files = {\n",
        "    \"edx_combined_data\": \"EDX_Combined_Data.csv\",\n",
        "    \"closed_us_power_plants\": \"Closed_US_Power_Plants.csv\",\n",
        "    \"eligible_48c_mfg_facilities\": \"Eligible_48C_Manufacturing_Facilities.csv\",\n",
        "    \"layer73_eligible_48c\": \"Layer73_Eligible_48C.csv\",\n",
        "    \"layer23_eligible_48c_not_ec\": \"Layer23_Eligible_48C_Not_Designated_EC.csv\",\n",
        "    \"filtered_us_coal_generators\": \"Filtered_US_Coal_Generators.csv\",\n",
        "    \"all_us_coal_generators\": \"All_US_Coal_Generators.csv\",\n",
        "    \"ffe_threshold_areas_csv\": \"FFE_Threshold_Areas.csv\",\n",
        "    \"energy_communities_2024_csv\": \"Energy_Communities_2024.csv\",\n",
        "    \"combined_48c_data_csv\": \"48C_Combined_Data.csv\",\n",
        "}\n",
        "\n",
        "all_dataframes = {}\n",
        "\n",
        "for name, filepath in manual_files.items():\n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"📥 Loading {filepath}...\")\n",
        "        all_dataframes[name] = pd.read_csv(filepath, low_memory=False)\n",
        "    else:\n",
        "        print(f\"⚠️ File not found: {filepath}\")\n",
        "\n",
        "# --- 2. Load NREL 238 submission (STEADy Solar Database)\n",
        "nrel_folder = \"nrel_steady_solar\"\n",
        "os.makedirs(nrel_folder, exist_ok=True)\n",
        "\n",
        "nrel_files = {\n",
        "    \"STEADy_Solar_Database.csv\": \"https://data.nrel.gov/system/files/238/STEADy%20Solar%20Database.csv\",\n",
        "    \"STEADy_Solar_Database.xlsx\": \"https://data.nrel.gov/system/files/238/STEADy%20Solar%20Database.xlsx\",\n",
        "}\n",
        "\n",
        "for filename, url in nrel_files.items():\n",
        "    dest_path = os.path.join(nrel_folder, filename)\n",
        "    if not os.path.exists(dest_path):\n",
        "        print(f\"⬇️ Downloading {filename}...\")\n",
        "        r = requests.get(url)\n",
        "        if r.status_code == 200:\n",
        "            with open(dest_path, \"wb\") as f:\n",
        "                f.write(r.content)\n",
        "            print(f\"✅ Saved {filename}\")\n",
        "        else:\n",
        "            print(f\"❌ Failed to download {filename}. Status code {r.status_code}\")\n",
        "\n",
        "# Load NREL 238 files\n",
        "try:\n",
        "    all_dataframes[\"steady_solar_database_csv\"] = pd.read_csv(os.path.join(nrel_folder, \"STEADy_Solar_Database.csv\"))\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Failed to load STEADy_Solar_Database.csv: {e}\")\n",
        "\n",
        "try:\n",
        "    all_dataframes[\"steady_solar_database_xlsx\"] = pd.read_excel(os.path.join(nrel_folder, \"STEADy_Solar_Database.xlsx\"), engine=\"openpyxl\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Failed to load STEADy_Solar_Database.xlsx: {e}\")\n",
        "\n",
        "print(\"\\n✅ All datasets are loaded into `all_dataframes` dictionary!\")\n",
        "\n",
        "# --- 3. Set index by year and state automatically if possible\n",
        "def auto_set_index(df, df_name):\n",
        "    # Make all column names lowercase\n",
        "    cols = [col.lower() for col in df.columns]\n",
        "    has_year = any('year' in col for col in cols)\n",
        "    has_state = any('state' in col for col in cols)\n",
        "\n",
        "    if has_year and has_state:\n",
        "        year_col = [col for col in df.columns if 'year' in col.lower()][0]\n",
        "        state_col = [col for col in df.columns if 'state' in col.lower()][0]\n",
        "        df = df.set_index([year_col, state_col])\n",
        "        print(f\"🔧 Set index for '{df_name}' using columns: {year_col}, {state_col}\")\n",
        "    else:\n",
        "        print(f\"⚠️ '{df_name}' does not have both 'year' and 'state' columns, skipping indexing.\")\n",
        "    return df\n",
        "\n",
        "# Apply to all dataframes\n",
        "for name in list(all_dataframes.keys()):\n",
        "    all_dataframes[name] = auto_set_index(all_dataframes[name], name)\n",
        "\n",
        "print(\"\\n🎉 Finished auto-indexing all datasets!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P66jdhr-gTe",
        "outputId": "f250d9ce-0822-4aca-9c2d-e13fce75b93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Loading EDX_Combined_Data.csv...\n",
            "📥 Loading Closed_US_Power_Plants.csv...\n",
            "📥 Loading Eligible_48C_Manufacturing_Facilities.csv...\n",
            "📥 Loading Layer73_Eligible_48C.csv...\n",
            "📥 Loading Layer23_Eligible_48C_Not_Designated_EC.csv...\n",
            "📥 Loading Filtered_US_Coal_Generators.csv...\n",
            "📥 Loading All_US_Coal_Generators.csv...\n",
            "📥 Loading FFE_Threshold_Areas.csv...\n",
            "📥 Loading Energy_Communities_2024.csv...\n",
            "📥 Loading 48C_Combined_Data.csv...\n",
            "⚠️ Failed to load STEADy_Solar_Database.csv: Error tokenizing data. C error: Expected 1 fields in line 10, saw 3\n",
            "\n",
            "⚠️ Failed to load STEADy_Solar_Database.xlsx: File is not a zip file\n",
            "\n",
            "✅ All datasets are loaded into `all_dataframes` dictionary!\n",
            "⚠️ 'edx_combined_data' does not have both 'year' and 'state' columns, skipping indexing.\n",
            "⚠️ 'closed_us_power_plants' does not have both 'year' and 'state' columns, skipping indexing.\n",
            "⚠️ 'eligible_48c_mfg_facilities' does not have both 'year' and 'state' columns, skipping indexing.\n",
            "⚠️ 'layer73_eligible_48c' does not have both 'year' and 'state' columns, skipping indexing.\n",
            "⚠️ 'layer23_eligible_48c_not_ec' does not have both 'year' and 'state' columns, skipping indexing.\n",
            "🔧 Set index for 'filtered_us_coal_generators' using columns: f_860m_operatingyear, f_860m_plantstate\n",
            "🔧 Set index for 'all_us_coal_generators' using columns: f_860m_operatingyear, f_860m_plantstate\n",
            "⚠️ 'ffe_threshold_areas_csv' does not have both 'year' and 'state' columns, skipping indexing.\n",
            "⚠️ 'energy_communities_2024_csv' does not have both 'year' and 'state' columns, skipping indexing.\n",
            "⚠️ 'combined_48c_data_csv' does not have both 'year' and 'state' columns, skipping indexing.\n",
            "\n",
            "🎉 Finished auto-indexing all datasets!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Assuming all your loaded DataFrames are stored in a dictionary like this\n",
        "# This dictionary should be the result from your earlier loading step\n",
        "all_dataframes = {\n",
        "    'edx_combined_data': edx_combined_data,\n",
        "    'closed_us_power_plants': closed_us_power_plants,\n",
        "    'eligible_48c_mfg_facilities': eligible_48c_mfg_facilities,\n",
        "    'layer73_eligible_48c': layer73_eligible_48c,\n",
        "    'layer23_eligible_48c_not_ec': layer23_eligible_48c_not_ec,\n",
        "    'filtered_us_coal_generators': filtered_us_coal_generators,\n",
        "    'all_us_coal_generators': all_us_coal_generators,\n",
        "    'ffe_threshold_areas_csv': ffe_threshold_areas_csv,\n",
        "    'energy_communities_2024_csv': energy_communities_2024_csv,\n",
        "    'combined_48c_data_csv': combined_48c_data_csv,\n",
        "    # Add steady_solar_database if it loaded correctly\n",
        "    # 'steady_solar_database': steady_solar_database,\n",
        "}\n",
        "\n",
        "# List to store the reshaped mini-dataframes\n",
        "final_rows = []\n",
        "\n",
        "# Function to find a \"state\" column (flexible)\n",
        "def find_state_column(df):\n",
        "    for col in df.columns:\n",
        "        if 'state' in col.lower():\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "# Function to find a \"year\" column (flexible)\n",
        "def find_year_column(df):\n",
        "    for col in df.columns:\n",
        "        if 'year' in col.lower():\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "# Process each dataframe\n",
        "for name, df in all_dataframes.items():\n",
        "    print(f\"\\n🔍 Processing {name}...\")\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Try to find 'year' and 'state' columns\n",
        "    state_col = find_state_column(df)\n",
        "    year_col = find_year_column(df)\n",
        "\n",
        "    if state_col is None:\n",
        "        df['state'] = np.nan\n",
        "    else:\n",
        "        df['state'] = df[state_col]\n",
        "\n",
        "    if year_col is None:\n",
        "        df['year'] = np.nan\n",
        "    else:\n",
        "        df['year'] = df[year_col]\n",
        "\n",
        "    # Drop extracted columns from the data part to avoid duplication\n",
        "    data_cols = [col for col in df.columns if col not in ['state', 'year']]\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        row_data = row[data_cols].to_dict()\n",
        "        final_rows.append({\n",
        "            'state': row['state'],\n",
        "            'year': row['year'],\n",
        "            'data': json.dumps(row_data, default=str)  # Serialize all other columns as JSON\n",
        "        })\n",
        "\n",
        "# Create the final mega-DataFrame\n",
        "final_df = pd.DataFrame(final_rows)\n",
        "\n",
        "print(\"\\n✅ All datasets have been processed and combined!\")\n",
        "print(final_df.head())\n",
        "\n",
        "# Optionally, save to CSV or Parquet\n",
        "final_df.to_csv(\"all_data_combined.csv\", index=False)\n",
        "# final_df.to_parquet(\"all_data_combined.parquet\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at97omAkBX2V",
        "outputId": "166871c9-04b3-4ae2-9fba-3f92542e72a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Processing edx_combined_data...\n",
            "\n",
            "🔍 Processing closed_us_power_plants...\n",
            "\n",
            "🔍 Processing eligible_48c_mfg_facilities...\n",
            "\n",
            "🔍 Processing layer73_eligible_48c...\n",
            "\n",
            "🔍 Processing layer23_eligible_48c_not_ec...\n",
            "\n",
            "🔍 Processing filtered_us_coal_generators...\n",
            "\n",
            "🔍 Processing all_us_coal_generators...\n",
            "\n",
            "🔍 Processing ffe_threshold_areas_csv...\n",
            "\n",
            "🔍 Processing energy_communities_2024_csv...\n",
            "\n",
            "🔍 Processing combined_48c_data_csv...\n",
            "\n",
            "✅ All datasets have been processed and combined!\n",
            "  state  year                                               data\n",
            "0   1.0   NaN  {\"AFFGEOID_Tract_2020\": \"1400000US01049960200\"...\n",
            "1   8.0   NaN  {\"AFFGEOID_Tract_2020\": \"1400000US08077001900\"...\n",
            "2   8.0   NaN  {\"AFFGEOID_Tract_2020\": \"1400000US08085966100\"...\n",
            "3  18.0   NaN  {\"AFFGEOID_Tract_2020\": \"1400000US18129040400\"...\n",
            "4  21.0   NaN  {\"AFFGEOID_Tract_2020\": \"1400000US21013960500\"...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"all_data_combined.csv\")\n"
      ],
      "metadata": {
        "id": "QAEUJILIE1_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Assuming your dataframe is called `df` and the third column is your JSON\n",
        "# Example: df.columns = [col0, col1, col2]\n",
        "\n",
        "# Step 1: Parse the JSON/dict if it isn't already\n",
        "df['parsed'] = df.iloc[:, 2].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Step 2: Extract 'year' and 'state' from the parsed dictionary\n",
        "df['Year'] = 2020  # fixed year because AFFGEOID_Tract_2020 is always 2020\n",
        "df['State'] = df['parsed'].apply(lambda x: x.get('State_Name'))\n",
        "\n",
        "# Step 3: Create new 'Data' column with the full dict (or you can drop State_Name from it if you want)\n",
        "df['Data'] = df['parsed']\n",
        "\n",
        "# Step 4: Select only three columns\n",
        "final_df = df[['Year', 'State', 'Data']]\n",
        "\n",
        "# Now `final_df` has exactly three columns\n",
        "print(final_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3U6gOwulFWzb",
        "outputId": "3920c80e-d648-44b8-83f6-6d735b3b2575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Year     State                                               Data\n",
            "0       2020   Alabama  {'AFFGEOID_Tract_2020': '1400000US01049960200'...\n",
            "1       2020  Colorado  {'AFFGEOID_Tract_2020': '1400000US08077001900'...\n",
            "2       2020  Colorado  {'AFFGEOID_Tract_2020': '1400000US08085966100'...\n",
            "3       2020   Indiana  {'AFFGEOID_Tract_2020': '1400000US18129040400'...\n",
            "4       2020  Kentucky  {'AFFGEOID_Tract_2020': '1400000US21013960500'...\n",
            "...      ...       ...                                                ...\n",
            "190140  2020       NaN  {'OID_': nan, 'CTract_GEO': nan, 'State_FIP': ...\n",
            "190141  2020       NaN  {'OID_': nan, 'CTract_GEO': nan, 'State_FIP': ...\n",
            "190142  2020       NaN  {'OID_': nan, 'CTract_GEO': nan, 'State_FIP': ...\n",
            "190143  2020       NaN  {'OID_': nan, 'CTract_GEO': nan, 'State_FIP': ...\n",
            "190144  2020       NaN  {'OID_': nan, 'CTract_GEO': nan, 'State_FIP': ...\n",
            "\n",
            "[190145 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Mapping of state abbreviations to full names\n",
        "state_lookup = {\n",
        "    \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\",\n",
        "    \"CA\": \"California\", \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DE\": \"Delaware\",\n",
        "    \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\",\n",
        "    \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\",\n",
        "    \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\", \"MD\": \"Maryland\",\n",
        "    \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\",\n",
        "    \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\",\n",
        "    \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\", \"NM\": \"New Mexico\", \"NY\": \"New York\",\n",
        "    \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\",\n",
        "    \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\",\n",
        "    \"SD\": \"South Dakota\", \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\",\n",
        "    \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\", \"WV\": \"West Virginia\",\n",
        "    \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\",\n",
        "    \"DC\": \"District of Columbia\",\n",
        "}\n",
        "\n",
        "def find_year(row):\n",
        "    for value in row.values():\n",
        "        if isinstance(value, int) and 1900 <= value <= 2100:\n",
        "            return value\n",
        "        if isinstance(value, str) and value.isdigit():\n",
        "            year = int(value)\n",
        "            if 1900 <= year <= 2100:\n",
        "                return year\n",
        "    return \"Unknown\"\n",
        "\n",
        "def find_state(row):\n",
        "    for value in row.values():\n",
        "        if isinstance(value, str):\n",
        "            val = value.strip().upper()\n",
        "            if val in state_lookup:\n",
        "                return state_lookup[val]\n",
        "            if val.title() in state_lookup.values():\n",
        "                return val.title()\n",
        "    return \"Federal\"\n",
        "\n",
        "def process_dataframe(df):\n",
        "    records = []\n",
        "    for _, row in df.iterrows():\n",
        "        row_dict = row.to_dict()\n",
        "        year = find_year(row_dict)\n",
        "        state = find_state(row_dict)\n",
        "        rest_of_data = row_dict  # could also remove year/state fields if needed\n",
        "        records.append({\n",
        "            \"year\": year,\n",
        "            \"state\": state,\n",
        "            \"rest_of_data\": rest_of_data\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# Example usage:\n",
        "processed_df = process_dataframe(final_df)\n",
        "processed_df.to_csv(\"processed_file.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "VGo3j1CYJsAn",
        "outputId": "7e85f20b-8767-4839-b48d-029e9b0f035c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-7bc7a14e58ca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mprocessed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mprocessed_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"processed_file.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-7bc7a14e58ca>\u001b[0m in \u001b[0;36mprocess_dataframe\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mrow_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0musing_cow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0musing_cow\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_single_block\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m                 \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_references\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.data_manager\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;31m# it is already ensured above this is not a NumpyExtensionArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# Until GH#49309 is fixed this check needs to come before the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/generic.py\u001b[0m in \u001b[0;36m_instancecheck\u001b[0;34m(cls, inst)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/generic.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(inst)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_pandas_abc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_typ\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_df.to_csv(\"processed_file.csv\", index=False)"
      ],
      "metadata": {
        "id": "xcMsGANLJ7-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('processed_file.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "J7DmRpvCL8g7",
        "outputId": "c4381457-d9a9-4ab2-c754-2b717c152951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_733d6761-7d57-4361-81ca-abc1dbc86ce4\", \"processed_file.csv\", 348340501)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}